{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bbbb830-6a2a-48d2-8c18-1f9c54339195",
   "metadata": {},
   "source": [
    "# Create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b9a3a1-8130-408f-83de-037180f27bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20027cf-d011-4876-b9f5-bd537c55a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    pre_tokenizers,\n",
    "    normalizers,\n",
    "    trainers,\n",
    "    decoders,\n",
    "    models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25329a9c-b4d3-4a04-848a-26a1dbb9adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a8267ba-38ba-4ea0-b7b9-ea212be31108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialTokensStr(Enum):\n",
    "    PAD = '[PAD]'\n",
    "    CLS = '[CLS]'\n",
    "    UNK = '[UNK]'\n",
    "    MASK = '[MASK]'\n",
    "    SOS = '[SOS]'\n",
    "    EOS = '[EOS]'\n",
    "\n",
    "    @classmethod\n",
    "    def todict(cls):\n",
    "        return {f'{token.name.lower()}_token': token.value for token in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def tolist(cls):\n",
    "        return list(cls.todict())\n",
    "\n",
    "class SpecialTokensInt(Enum):\n",
    "    PAD = 0\n",
    "    CLS = 1\n",
    "    UNK = 2\n",
    "    MASK = 3\n",
    "    SOS = 4\n",
    "    EOS = 5\n",
    "\n",
    "    @classmethod\n",
    "    def todict(cls):\n",
    "        return {token.name: token.value for token in cls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18322d3b-8b39-4844-ab27-dd9360dd573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TokenizerImDB:\n",
    "    vocab_size: int\n",
    "    tokenizer_path: Path\n",
    "    tokenizer: Tokenizer = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.tokenizer_path.exists():\n",
    "            self.load_tokenizer()\n",
    "\n",
    "    def train(self, text_iterator):\n",
    "        self.tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "        self.tokenizer.normalizer = normalizers.Sequence(\n",
    "            [\n",
    "                normalizers.Lowercase(),\n",
    "                normalizers.Strip(),\n",
    "                normalizers.NFC(),\n",
    "                normalizers.NFD(),\n",
    "                normalizers.NFKC(),\n",
    "                normalizers.NFKD(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "        self.tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "        special_tokens = SpecialTokensStr.tolist()\n",
    "\n",
    "        self.trainer = trainers.BpeTrainer(\n",
    "            vocab_size=self.vocab_size, \n",
    "            special_tokens=special_tokens\n",
    "        )\n",
    "\n",
    "        self.tokenizer.train_from_iterator(text_iterator, self.trainer)\n",
    "\n",
    "        self.tokenizer.save(str(self.tokenizer_path))\n",
    "\n",
    "    def encoder(self, text: str, **kwargs) -> list[int]:\n",
    "        return self.tokenizer.encode(text, **kwargs).ids\n",
    "\n",
    "    def decoder(self, ids: list[int]) -> str:\n",
    "        return self.tokenizer.decode(ids)\n",
    "\n",
    "    def load_tokenizer(self):\n",
    "        self.tokenizer = Tokenizer.from_file(str(self.tokenizer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f460994-acb4-4e1a-9ff9-2c36060e46df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens Sting: {'pad_token': '[PAD]', 'cls_token': '[CLS]', 'unk_token': '[UNK]', 'mask_token': '[MASK]', 'sos_token': '[SOS]', 'eos_token': '[EOS]'}\n",
      "Special Tokens Indices: {'PAD': 0, 'CLS': 1, 'UNK': 2, 'MASK': 3, 'SOS': 4, 'EOS': 5}\n"
     ]
    }
   ],
   "source": [
    "print(f'Special Tokens Sting: {SpecialTokensStr.todict()}')\n",
    "print(f'Special Tokens Indices: {SpecialTokensInt.todict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caae16dd-c69b-4668-ad87-e415b99debb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dataset = Path('../data/imdb-reviews-pt-br.csv')\n",
    "dataset = pd.read_csv(file_dataset)\n",
    "\n",
    "def text_iterator(dataset: pd.DataFrame, language: str):\n",
    "    match language:\n",
    "        case 'pt':\n",
    "            text_col = dataset['text_pt']\n",
    "        case 'en':\n",
    "            text_col = dataset['text_en']\n",
    "    for text in text_col:\n",
    "        yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b892f94c-13a8-4654-b402-8e290846ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path_pt = Path('artifacts/tokenizer_pt.json')\n",
    "tokenizer_path_en = Path('artifacts/tokenizer_en.json')\n",
    "\n",
    "tokenizer_pt = TokenizerImDB(vocab_size=VOCAB_SIZE, tokenizer_path=tokenizer_path_pt)\n",
    "tokenizer_en = TokenizerImDB(vocab_size=VOCAB_SIZE, tokenizer_path=tokenizer_path_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf3879-4331-4946-87d1-f2974093767e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "203ef8e9-65ee-4cda-817d-e4be3ad53c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_pt.train(text_iterator(dataset, 'pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe78dc4c-9104-43ab-8e69-d10a5502f03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en.train(text_iterator(dataset, 'en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db5c9a-7721-4c29-94bf-c190eb93ca19",
   "metadata": {},
   "source": [
    "# Loaded tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1a72f5b-bff6-4c86-a9a6-93560bdd1a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_pt = TokenizerImDB(vocab_size=VOCAB_SIZE, tokenizer_path=tokenizer_path_pt)\n",
    "tokenizer_en = TokenizerImDB(vocab_size=VOCAB_SIZE, tokenizer_path=tokenizer_path_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80238a-2196-4b76-be50-72a5dac1bf4f",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ceb135a-3382-4974-bf9e-ba7ddde6e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pt = 'Olá como vai você?'\n",
    "text_en = 'Hello, how are you?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bf3d13b-fae4-4e7f-bea6-5dfa06edd564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13344, 125, 218, 660, 259, 3878]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_pt.encoder(text_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a88dc57b-735c-4892-bab4-2e7e3b230716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15752, 13, 368, 222, 199, 32]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.encoder(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa3bff8a-0a23-4aa8-b18e-ce122537f290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'olá como vai você?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_pt.decoder(tokenizer_pt.encoder(text_pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78404dec-ee77-4d3a-b3be-e64edb4157de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, how are you?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.decoder(tokenizer_en.encoder(text_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7f2285a-31bc-433f-9e43-0487fcffa959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13344, 125, 218, 660, 259, 3878]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_pt.encoder(tokenizer_pt.decoder(tokenizer_pt.encoder(text_pt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "701436eb-b51f-47ed-8b2c-47d7936f5d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15752, 13, 368, 222, 199, 32]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.encoder(tokenizer_en.decoder(tokenizer_en.encoder(text_en)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806114e-8541-4885-833d-85d0b71354c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
