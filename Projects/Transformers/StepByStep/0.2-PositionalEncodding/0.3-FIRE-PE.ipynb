{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c6df4b-a82f-4193-bec4-0a250a566c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FIRE(nn.Module):\n",
    "    def __init__(self, num_heads=12, mlp_width=32, init_c=0.1,\n",
    "                 init_L=512., eps=1e-6):\n",
    "        \"\"\"\n",
    "        FIRE attention bias module.\n",
    "\n",
    "        Args:\n",
    "            num_heads: number of attention heads.\n",
    "            mlp_width: Width of MLP.\n",
    "            init_c: initial value of log transformation parameter.\n",
    "            init_L: initial value of thresholding parameter.\n",
    "            eps: small constant for numerical stability.\n",
    "        \"\"\"\n",
    "        super(FIRE, self).__init__()\n",
    "\n",
    "        # Define the MLP layers\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(1, mlp_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_width, num_heads)\n",
    "        )\n",
    "\n",
    "        # Initialize c (log transformation parameter)\n",
    "        self.c = nn.Parameter(torch.tensor(init_c), requires_grad=False)\n",
    "\n",
    "        # Initialize L (threshold)\n",
    "        self.init_L = nn.Parameter(torch.tensor(init_L), requires_grad=False)\n",
    "        # Learn a multiplier to L\n",
    "        self.L_multiplier = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute FIRE attention bias.\n",
    "\n",
    "        Args:\n",
    "            x: input sequence, shape [bsz, num_heads, seq_len, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            attention bias, shape [1, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        seq_length = x.size(2)\n",
    "        positions = torch.arange(seq_length, dtype=torch.float, device=x.device)\n",
    "        rel_distance = positions[:, None] - positions[None, :]\n",
    "\n",
    "        # Thresholding the normalizer\n",
    "        threshold = torch.abs(self.L_multiplier * self.init_L)\n",
    "        pos_normalizer = torch.max(positions, threshold)\n",
    "        pos_normalizer = pos_normalizer[:, None]\n",
    "\n",
    "        # Amplifying differences among local positions with log transform\n",
    "        rel_distance = torch.log(torch.abs(self.c * rel_distance) + 1)\n",
    "        pos_normalizer = torch.log(torch.abs(self.c * pos_normalizer) + 1) + self.eps\n",
    "\n",
    "        # Progressive interpolation\n",
    "        normalized_distance = rel_distance / pos_normalizer\n",
    "        fire_bias = self.mlp(normalized_distance.unsqueeze(-1))\n",
    "        fire_bias = fire_bias.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "        return fire_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd24bfe-9536-463d-90dc-c95c2acf3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithFIRE(nn.Module):\n",
    "    def __init__(self, dim_model, num_heads):\n",
    "        super(AttentionWithFIRE, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(dim_model, num_heads)\n",
    "        self.positional_encoding = FIRE(dim_model)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # Get query and key positions\n",
    "        query_pos = torch.arange(queries.size(0))\n",
    "        key_pos = torch.arange(keys.size(0))\n",
    "        \n",
    "        # Compute FIRE positional encoding\n",
    "        pos_encoding = self.positional_encoding(query_pos, key_pos)\n",
    "        \n",
    "        # Add positional encoding to attention scores\n",
    "        attn_output, attn_weights = self.attention(queries + pos_encoding, keys, values)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986051cf-70fa-4e2a-b8dc-ce529a183bdf",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad092daa-fa36-450c-a8c5-bf37e94cb1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# FIRE module as provided\n",
    "class FIRE(nn.Module):\n",
    "    def __init__(self, num_heads=12, mlp_width=32, init_c=0.1, init_L=512., eps=1e-6):\n",
    "        super(FIRE, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(1, mlp_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_width, num_heads)\n",
    "        )\n",
    "        self.c = nn.Parameter(torch.tensor(init_c))\n",
    "        self.init_L = nn.Parameter(torch.tensor(init_L), requires_grad=False)\n",
    "        self.L_multiplier = nn.Parameter(torch.tensor(1.0))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq_length = x.size(2)\n",
    "        positions = torch.arange(seq_length, dtype=torch.float, device=x.device)\n",
    "        rel_distance = positions[:, None] - positions[None, :]\n",
    "\n",
    "        # Thresholding the normalizer\n",
    "        threshold = torch.abs(self.L_multiplier * self.init_L)\n",
    "        pos_normalizer = torch.max(positions, threshold)\n",
    "        pos_normalizer = pos_normalizer[:, None]\n",
    "\n",
    "        # Amplifying differences among local positions with log transform\n",
    "        rel_distance = torch.log(torch.abs(self.c * rel_distance) + 1)\n",
    "        pos_normalizer = torch.log(torch.abs(self.c * pos_normalizer) + 1) + self.eps\n",
    "\n",
    "        # Progressive interpolation\n",
    "        normalized_distance = rel_distance / pos_normalizer\n",
    "        fire_bias = self.mlp(normalized_distance.unsqueeze(-1))\n",
    "        fire_bias = fire_bias.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "        return fire_bias\n",
    "\n",
    "# Multihead Attention with FIRE integration\n",
    "class MultiheadAttentionWithFIRE(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, fire_params=None):\n",
    "        super(MultiheadAttentionWithFIRE, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        # Define projections\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Define FIRE module\n",
    "        self.fire = FIRE(**fire_params) if fire_params else FIRE(num_heads=num_heads)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        bsz, seq_len, embed_dim = query.size()\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.q_proj(query).view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(key).view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(value).view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        scores = torch.einsum(\"bhqd, bhkd -> bhqk\", Q, K) / self.head_dim ** 0.5\n",
    "\n",
    "        # Apply FIRE bias\n",
    "        fire_bias = self.fire(Q)\n",
    "        scores = scores + fire_bias\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # Attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Compute attention output\n",
    "        output = torch.einsum(\"bhqk, bhvd -> bhqd\", attn_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seq_len, embed_dim)\n",
    "\n",
    "        # Final linear projection\n",
    "        return self.out_proj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2bb3ca6-c3ad-4eeb-bbd3-cbe0eb79ac72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Example parameters\n",
    "embed_dim = 6\n",
    "num_heads = 2\n",
    "fire_params = {'num_heads': num_heads, 'mlp_width': 32}\n",
    "\n",
    "# Instantiate the model\n",
    "attention_layer = MultiheadAttentionWithFIRE(embed_dim, num_heads, fire_params)\n",
    "\n",
    "# Dummy input\n",
    "query = torch.rand(2, 5, embed_dim)  # [batch_size, seq_len, embed_dim]\n",
    "key = torch.rand(2, 5, embed_dim)\n",
    "value = torch.rand(2, 5, embed_dim)\n",
    "\n",
    "# Forward pass\n",
    "output = attention_layer(query, key, value)\n",
    "print(output.shape)  # Should be [16, 10, 64]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0fd6e2a-9bdb-4045-afe8-75ef08d07dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7272, -2.0209, -1.0937,  0.3331, -0.3517, -0.3415],\n",
       "         [ 0.7272, -2.0209, -1.0937,  0.3331, -0.3517, -0.3415],\n",
       "         [ 0.7272, -2.0209, -1.0937,  0.3331, -0.3517, -0.3415],\n",
       "         [ 0.7272, -2.0209, -1.0937,  0.3331, -0.3517, -0.3415],\n",
       "         [ 0.7272, -2.0209, -1.0937,  0.3331, -0.3517, -0.3415]],\n",
       "\n",
       "        [[ 0.7759, -1.8854, -0.9876,  0.2124, -0.4803, -0.3230],\n",
       "         [ 0.7759, -1.8854, -0.9876,  0.2124, -0.4803, -0.3230],\n",
       "         [ 0.7759, -1.8854, -0.9876,  0.2124, -0.4803, -0.3230],\n",
       "         [ 0.7759, -1.8854, -0.9876,  0.2124, -0.4803, -0.3230],\n",
       "         [ 0.7759, -1.8854, -0.9876,  0.2124, -0.4803, -0.3230]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c875e13-094b-4848-9a4e-5678451841a3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a74d1a1-6ec2-41c6-be71-8aa6af75f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Dot product of query and key (transpose last two dimensions of key)\n",
    "        attn = torch.matmul(query, key.transpose(-2, -1)) / self.temperature\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand_as(attn)  # Expanding mask for all heads.\n",
    "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        output = torch.matmul(attn, value)\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=0.02)\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0, std=0.02)\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0, std=0.02)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        sz_b, len_q, _ = q.size()\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        # Pass through the pre-attention projection: query, key, value are all split into multiple heads\n",
    "        qs = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        ks = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        vs = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        qs, ks, vs = qs.transpose(1, 2), ks.transpose(1, 2), vs.transpose(1, 2)  # [sz_b, n_head, len_q, d_k]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # [sz_b, 1, 1, len_k]\n",
    "\n",
    "        # Apply attention on all the projected vectors in batch\n",
    "        outputs, attn = self.attention(qs, ks, vs, mask=mask)\n",
    "\n",
    "        # Concatenate heads and project back to original size\n",
    "        outputs = outputs.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        outputs = self.dropout(self.fc(outputs))\n",
    "        outputs += q\n",
    "        outputs = self.layer_norm(outputs)\n",
    "\n",
    "        return outputs, attn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_width, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(\n",
    "            n_head=num_heads, d_model=embed_dim, \n",
    "            d_k=embed_dim // num_heads, d_v=embed_dim // num_heads, \n",
    "            dropout=dropout_rate)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.fire = FIRE(num_heads=num_heads, mlp_width=mlp_width)\n",
    "\n",
    "    def forward(self, src):\n",
    "        attn_bias = self.fire(src)\n",
    "        \n",
    "        # Ensure the mask is appropriate for multi-head attention.\n",
    "        attn_bias = attn_bias.repeat(src.size(0), 1, 1, 1)  # Make sure the mask has the batch dimension where necessary\n",
    "        attn_bias = (attn_bias <= 0).to(torch.float32)  # Creating the actual mask\n",
    "\n",
    "        attn_output, _ = self.attention(src, src, src, mask=attn_bias)\n",
    "        attn_output = self.norm1(attn_output)\n",
    "\n",
    "        ff_output = self.feed_forward(attn_output)\n",
    "        output = attn_output + ff_output\n",
    "        output = self.norm2(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_width, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Manually implemented multi-head attention\n",
    "        self.attention = MultiHeadAttention(n_head=num_heads, d_model=embed_dim, d_k=embed_dim // num_heads, d_v=embed_dim // num_heads, dropout=dropout_rate)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Feed-forward network as part of the Transformer block\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # FIRE encoder for position encoding\n",
    "        self.fire = FIRE(num_heads=num_heads, mlp_width=mlp_width)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Apply FIRE positional encoding as an additive bias in attention\n",
    "        attn_bias = self.fire(src)\n",
    "        attn_output, _ = self.attention(src, src, src, mask=attn_bias)\n",
    "        attn_output = self.norm1(attn_output)  # Layer norm after addition and attention\n",
    "\n",
    "        ff_output = self.feed_forward(attn_output)\n",
    "        output = attn_output + ff_output  # Residual connection\n",
    "        output = self.norm2(output)  # Layer norm after addition\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d299a100-c3fd-4c58-b196-b2f26e30abe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[1, 1, 1, 8, 128, 128]}, size=[2, 8, 10, 10]): the number of sizes provided (4) must be greater or equal to the number of dimensions in the tensor (6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, seq_length, embed_dim)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Pass the input through the Transformer model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Display the output\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput Tensor Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 133\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src):\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Apply FIRE positional encoding as an additive bias in attention\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     attn_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfire(src)\n\u001b[0;32m--> 133\u001b[0m     attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(attn_output)  \u001b[38;5;66;03m# Layer norm after addition and attention\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     ff_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(attn_output)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 65\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     62\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# [sz_b, 1, 1, len_k]\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Apply attention on all the projected vectors in batch\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m outputs, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Concatenate heads and project back to original size\u001b[39;00m\n\u001b[1;32m     68\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(sz_b, len_q, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mScaledDotProductAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m attn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query, key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Expanding mask for all heads.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     18\u001b[0m attn \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attn, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[1, 1, 1, 8, 128, 128]}, size=[2, 8, 10, 10]): the number of sizes provided (4) must be greater or equal to the number of dimensions in the tensor (6)"
     ]
    }
   ],
   "source": [
    "# Define the Transformer model parameters\n",
    "embed_dim = 128  # Embedding dimension\n",
    "num_heads = 8    # Number of attention heads\n",
    "mlp_width = 256  # Width of the MLP in the FIRE module\n",
    "seq_length = 10  # Length of the input sequence\n",
    "batch_size = 2   # Number of sequences in a batch\n",
    "\n",
    "# Instantiate the TransformerBlock\n",
    "transformer_block = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, mlp_width=mlp_width)\n",
    "\n",
    "# Create a sample input tensor\n",
    "# Assume input tensor shape [batch_size, seq_length, embed_dim]\n",
    "# Random tensor mimicking a batch of embedded sequences\n",
    "input_tensor = torch.randn(batch_size, seq_length, embed_dim)\n",
    "\n",
    "# Pass the input through the Transformer model\n",
    "output = transformer_block(input_tensor)\n",
    "\n",
    "# Display the output\n",
    "print(\"Output Tensor Shape:\", output.shape)\n",
    "print(\"Output Tensor:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a2084-edfd-4b6a-b6c0-a74c00954933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
