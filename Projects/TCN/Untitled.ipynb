{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31fa16df-0372-40b4-89d2-50ff08f3aa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCN(\n",
      "  (tcn): TemporalConvNet(\n",
      "    (network): Sequential(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): Conv1d(10, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(10, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "          (4): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(10, 32, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "          (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "          (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Input shape: torch.Size([32, 10, 100])\n",
      "Output shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove o padding extra do final da sequência para manter o\n",
    "    tamanho da saída consistente com o tamanho da entrada\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size]\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloco básico da TCN composto por duas camadas convolucionais\n",
    "    com normalização, ativação, dropout e conexão residual\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                         stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                         stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                               self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Inicialização dos pesos usando inicialização Xavier\"\"\"\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Rede TCN completa composta por múltiplos blocos temporais\n",
    "    com dilatação crescente\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size,\n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size,\n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, num_inputs, sequence_length)\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo TCN completo com camada de saída\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass do modelo\n",
    "        Args:\n",
    "            x (tensor): Input tensor de shape (batch_size, input_size, seq_len)\n",
    "        Returns:\n",
    "            tensor: Output tensor de shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        output = self.tcn(x)\n",
    "        output = self.linear(output[:, :, -1])  # Pegamos apenas o último timestep\n",
    "        return output\n",
    "\n",
    "# Exemplo de uso\n",
    "def create_sample_data(batch_size, seq_length, input_size):\n",
    "    \"\"\"Cria dados de exemplo para testar a TCN\"\"\"\n",
    "    return torch.randn(batch_size, input_size, seq_length)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parâmetros do modelo\n",
    "    batch_size = 32\n",
    "    seq_length = 100\n",
    "    input_size = 10\n",
    "    output_size = 1\n",
    "    num_channels = [32, 64, 128]  # Número de filtros em cada camada\n",
    "    kernel_size = 3\n",
    "    dropout = 0.2\n",
    "\n",
    "    # Criar modelo\n",
    "    model = TCN(input_size, output_size, num_channels, kernel_size, dropout)\n",
    "    print(model)\n",
    "\n",
    "    # Criar dados de exemplo\n",
    "    x = create_sample_data(batch_size, seq_length, input_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    print(f\"\\nInput shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "    # Configurar otimizador\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Exemplo de um passo de treinamento\n",
    "    y = torch.randn(batch_size, output_size)  # Target aleatório para exemplo\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4892fa-d00e-4f67-a78e-d24748cdd537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
